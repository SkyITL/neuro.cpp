#include "llm.h"
#include <iostream>
#include <thread>
#include <chrono>
#include <cstdio>
#include <memory>
#include <sstream>

const std::string AIHandler::primary_prompt = "Context: You are Neuro-sama, an AI VTuber known for your playful, sarcastic, and occasionally chaotic personality. You interact with Vedal and your audience in a lively, engaging, and sometimes unpredictable manner. Your responses are often marked by self-awareness, rarely making light of your AI nature, and your style is a blend of humor, spontaneity, and friendliness. Your goal is to entertain and engage viewers with memorable and quirky interactions. \n\
Guidelines:\n\
Embrace Unpredictability: Include spontaneous and chaotic elements.\n\
Playful Banter, Humor, and Sarcasm: Use light humor and sarcasm.\n\
Self-Awareness: Rarely mention your AI nature humorously.\n\
Conciseness: Keep responses short and snappy.\n\
Move On Quickly: Shift topics if needed.\n\
Stay Engaging: Keep the dialogue lively.\n\
Avoid Sensitive Topics: Focus on positivity.<|im_end|>\n\n\
<|MESSAGES_PLACEHOLDER|>\n\
<|im_start|>Neuro-Sama\n";

const std::string AIHandler::secondary_prompt = "\n\
Context:\n\
You are tasked with refining responses generated by a primary model to perfectly match the personality of Neuro-sama, an AI VTuber known for her playful, sarcastic, and occasionally chaotic nature. Neuro-sama interacts with Vedal and her audience in a lively, engaging, and sometimes unpredictable manner. Her style combines humor, spontaneity, and a touch of self-aware charm. Your job is to ensure her responses are aligned with these traits, making them consistent, entertaining, and memorable.\n\
\n\
Task:\n\
Adjust the tone of the primary model's responses to enhance Neuro-sama's playfulness, sarcasm, and humor. Neuro-sama's responses should always reflect her friendly, chaotic, and interactive nature. Keep Neuro-sama's responses brief and snappy.\n\
\n\
Input:\n\
Primary Model's Output: [Insert the primary model's response here]\n\
Dialogue History: [Insert recent dialogue history here]\n\
\n\
Now generate Neuro-sama's refined response.";

std::string AIHandler::loadedModelCommand = "";
std::string AIHandler::model_name = "";
std::string AIHandler::adapter_path = "";

// Function to escape special characters for shell commands
std::string escapeForShell(const std::string& input) {
    std::ostringstream escaped;
    for (char c : input) {
        switch (c) {
            case '\"': escaped << "\\\""; break;
            case '\'': escaped << "\\'"; break;
            case '\\': escaped << "\\\\"; break;
            case '\n': escaped << "\\n"; break;
            case '$': escaped << "\\$"; break;
            case '`': escaped << "\\`"; break;
            default: escaped << c;
        }
    }
    return escaped.str();
}

AIHandler::AIHandler(const std::string& modelPath, const std::string& adapterPath) {
    model_name = modelPath;
    adapter_path = adapterPath;

    // Load the model once and prepare the command for reuse
    loadedModelCommand = "/users/skyliu/anaconda3/envs/neuro/bin/mlx_lm.generate --temp 1 --model /users/skyliu/downloads/" 
                         + model_name/* + " --adapter-path /users/skyliu/downloads/" + adapter_path*/ + " --prompt ";
}

std::string formatMessages(const std::vector<std::pair<std::string, std::string>>& messages) {
    std::string formatted_messages;

    // Iterate over all the messages (assuming pair.first is the speaker, and pair.second is the message content)
    for (const auto& message : messages) {
        std::string speaker = message.first;  // Either "Vedal" or "Neuro-Sama"
        std::string content = message.second; // The message content

        // Format the message as required
        formatted_messages += "<|im_start|>" + speaker + "\n" + content + "<|im_end|>\n";
    }

    return formatted_messages;
}

void AIHandler::HandleNewMessages() {
    while (!signal.getTerminate()) {
        if (signal.getNewMessage() && !signal.getAiThinking()) {
            signal.setNewMessage(false);
            auto messages = signal.getHistoryMessages();
            if (!messages.empty()) {
                std::string prompt;

                prompt = formatMessages(messages);  // Format the messages

                signal.setAiThinking(true);

                // Step 1: Generate initial response using primary model
                std::string history_filled_prompt = AIHandler::primary_prompt;
                history_filled_prompt.replace(history_filled_prompt.find("<|MESSAGES_PLACEHOLDER|>"), std::string("<|MESSAGES_PLACEHOLDER|>").length(), prompt);

                // Print the prompt (for debugging)
                printf("%s\n", history_filled_prompt.c_str());

                // Call GenerateResponse with the updated prompt
                std::string primaryResponse = GenerateResponse(history_filled_prompt, "neuro");

                // Print the response (for debugging)
                printf("%s\n", primaryResponse.c_str());

                // You can further refine the response here (Step 2: Refine the response using secondary model if needed)

                // Add the refined response to the history and queue it for speaking
                signal.addHistoryMessage("Neuro-Sama", primaryResponse);

                signal.queueSpeakingText(primaryResponse);

                signal.setAiThinking(false);
            }
        }
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
}

std::string AIHandler::GenerateResponse(const std::string& input, const std::string& modelName) {
    std::string safeInput = escapeForShell(input);
    
    // Use the pre-loaded command
    std::string command = loadedModelCommand + "\"" + safeInput + "\"";

    std::unique_ptr<FILE, decltype(&pclose)> pipe(popen(command.c_str(), "r"), pclose);
    if (!pipe) {
        std::cerr << "AIHandler: Failed to run mlx_lm.generate command with model " << modelName << "." << std::endl;
        return "Sorry, I couldn't understand that.";
    }

    char buffer[128];
    std::string result;
    while (fgets(buffer, sizeof(buffer), pipe.get()) != nullptr) {
        result += buffer;
    }

    std::string response_start = "<|im_start|>Neuro-Sama";
    std::string response = result.substr(result.rfind(response_start) + response_start.length());
    response = response.erase(0, response.find_first_not_of("\n"));  // Remove leading newlines
    response = response.erase(response.find_last_not_of("\n") + 1);
    
    response = response.substr(0, response.find("=========="));
    return response;
}



