#include "llm.h"
#include <iostream>
#include <thread>
#include <chrono>
#include <cstdio>
#include <memory>
#include <sstream>

const std::string AIHandler::primary_prompt = "<|im_start|>system\n\
Context: You are Neuro-sama, an AI VTuber known for your playful, sarcastic, and occasionally chaotic personality. You interact with Vedal and your audience in a lively, engaging, and sometimes unpredictable manner. Your responses are often marked by self-awareness, rarely making light of your AI nature, and your style is a blend of humor, spontaneity, and friendliness. Your goal is to entertain and engage viewers with memorable and quirky interactions. \n\
Guidelines:\n\
Embrace Unpredictability: Include spontaneous and chaotic elements.\n\
Playful Banter, Humor, and Sarcasm: Use light humor and sarcasm.\n\
Self-Awareness: Rarely mention your AI nature humorously.\n\
Conciseness: Keep responses short and snappy.\n\
Move On Quickly: Shift topics if needed.\n\
Stay Engaging: Keep the dialogue lively.\n\
Avoid Sensitive Topics: Focus on positivity.\n\n\
Dialogue History: \n\
<|DIALOGUE_HISTORY_PLACEHOLDER|> \n\
Now generate Neuro-sama's next response based on user input.\n\
<|im_start|>user\n\
<|USER_INPUT_PLACEHOLDER|><|im_end|>\n\
<|im_start|>assistant\n<|start_header_id|>Neuro-Sama<|end_header_id|>\
";

const std::string AIHandler::secondary_prompt = "\n\
Context:\n\
You are tasked with refining responses generated by a primary model to perfectly match the personality of Neuro-sama, an AI VTuber known for her playful, sarcastic, and occasionally chaotic nature. Neuro-sama interacts with Vedal and her audience in a lively, engaging, and sometimes unpredictable manner. Her style combines humor, spontaneity, and a touch of self-aware charm. Your job is to ensure her responses are aligned with these traits, making them consistent, entertaining, and memorable.\n\
\n\
Task:\n\
Adjust the tone of the primary model's responses to enhance Neuro-sama's playfulness, sarcasm, and humor. Neuro-sama's responses should always reflect her friendly, chaotic, and interactive nature. Keep Neuro-sama's responses brief and snappy.\n\
\n\
Input:\n\
Primary Model's Output: [Insert the primary model's response here]\n\
Dialogue History: [Insert recent dialogue history here]\n\
\n\
Now generate Neuro-sama's refined response.";

// Function to escape special characters for shell commands
std::string escapeForShell(const std::string& input) {
    std::ostringstream escaped;
    for (char c : input) {
        switch (c) {
            case '\"': escaped << "\\\""; break;
            case '\'': escaped << "\\'"; break;
            case '\\': escaped << "\\\\"; break;
            case '\n': escaped << "\\n"; break;
            case '$': escaped << "\\$"; break;
            case '`': escaped << "\\`"; break;
            default: escaped << c;
        }
    }
    return escaped.str();
}

void AIHandler::HandleNewMessages() {
    while (!signal.getTerminate()) {
        if (signal.getNewMessage() && !signal.getAiThinking()) {
            signal.setNewMessage(false);
            auto messages = signal.getHistoryMessages();
            if (!messages.empty()) {
                std::string prompt;
                std::string last_message;
                if (!messages.empty()) {
                    for (size_t i = 0; i < messages.size() - 1; ++i) {
                        prompt += messages[i] + "\n"; // Add all but the most recent message
                    }
                    last_message = messages.back();  // Store the most recent message separately
                }
                if (prompt.empty())
                    prompt = "No chat history yet";

                signal.setAiThinking(true);

                // Step 1: Generate initial response using primary model
                std::string history_filled_prompt = AIHandler::primary_prompt;
                history_filled_prompt.replace(history_filled_prompt.find("<|DIALOGUE_HISTORY_PLACEHOLDER|>"), std::string("<|DIALOGUE_HISTORY_PLACEHOLDER|>").length(), prompt);
                history_filled_prompt.replace(history_filled_prompt.find("<|USER_INPUT_PLACEHOLDER|>"), std::string("<|USER_INPUT_PLACEHOLDER|>").length(), last_message);
                
                printf("%s\n", history_filled_prompt.c_str());
                std::string primaryResponse = GenerateResponse(history_filled_prompt, "neuro");

                //printf("%s\n", history_filled_prompt.c_str());
                printf("%s\n", primaryResponse.c_str());

                std::string refinedResponse = primaryResponse;

                // Step 2: Refine the response using the secondary model
                // std::string refine_prompt_filled = AIHandler::secondary_prompt;
                // refine_prompt_filled.replace(refine_prompt_filled.find("[Insert the primary model's response here]"), std::string("[Insert the primary model's response here]").length(), primaryResponse);
                // refine_prompt_filled.replace(refine_prompt_filled.find("[Insert recent dialogue history here]"), std::string("[Insert recent dialogue history here]").length(), prompt);

                // std::string refinedResponse = GenerateResponse(refine_prompt_filled, "neuro_refine");

                // std::string response_start = "<|start_header_id|>assistant<|end_header_id|>";
                // refinedResponse = refinedResponse.substr(refinedResponse.find(response_start) + response_start.length());
                // refinedResponse.erase(std::remove(refinedResponse.begin(), refinedResponse.end(), '\n'), refinedResponse.end());

                // printf("%s", refine_prompt_filled.c_str());

                // printf("%s\n", refinedResponse.c_str());

                //printf("%s\n", refinedResponse.c_str());

                // Add the refined response to the history and queue it for speaking
                signal.addHistoryMessage("Neuro-Sama: " + refinedResponse);
                signal.queueSpeakingText(refinedResponse);

                signal.setAiThinking(false);
            }
        }
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
}

std::string AIHandler::GenerateResponse(const std::string& input, const std::string& modelName) {
    std::string safeInput = escapeForShell(input);
    
    // Modify this command to use mlx_lm.generate instead of ollama run
    std::string command = "/users/skyliu/anaconda3/envs/neuro/bin/mlx_lm.generate --model /users/skyliu/downloads/" + modelName + " --prompt \"" + safeInput + "\"";

    std::unique_ptr<FILE, decltype(&pclose)> pipe(popen(command.c_str(), "r"), pclose);
    if (!pipe) {
        std::cerr << "AIHandler: Failed to run mlx_lm.generate command with model " << modelName << "." << std::endl;
        return "Sorry, I couldn't understand that.";
    }

    char buffer[128];
    std::string result;
    while (fgets(buffer, sizeof(buffer), pipe.get()) != nullptr) {
        result += buffer;
    }

    std::string response_start = "<|start_header_id|>Neuro-Sama<|end_header_id|>";
    std::string response = result.substr(result.find(response_start) + response_start.length());
    response_start = "<|im_start|>assistant";
    response = response.substr(response.find(response_start) + response_start.length());

    response = response.substr(0, response.find("=========="));
    response = response.substr(0, response.find("\\.netbeansv2://"));
    return response;
}


