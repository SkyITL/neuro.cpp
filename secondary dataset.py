import json
import mlx_lm.manage
# Paths for validation data and output files
valid_data_path = '/Users/skyliu/documents/llama3/neuro_data/v4/valid.jsonl'  # Path to your validation dataset
primary_output_path = '/Users/skyliu/neurorawdata/primary_model_output.jsonl'  # Where to save primary model outputs
secondary_data_path = '/Users/skyliu/neurorawdata/secondary_model_data.jsonl'  # Path for secondary model dataset

secondary_system_prompt = ("\n\
Context:\n\
You are tasked with refining responses generated by a primary model to perfectly match the personality of Neuro-sama, an AI VTuber known for her playful, sarcastic, and occasionally chaotic nature. Neuro-sama interacts with Vedal and her audience in a lively, engaging, and sometimes unpredictable manner. Her style combines humor, spontaneity, and a touch of self-aware charm. Your job is to ensure her responses are aligned with these traits, making them consistent, entertaining, and memorable.\n\
\n\
Task:\n\
Adjust the tone of the primary model's responses to enhance Neuro-sama's playfulness, sarcasm, and humor. Neuro-sama's responses should always reflect her friendly, chaotic, and interactive nature. Keep Neuro-sama's responses brief and snappy.\n\
\n\
Input:\n\
Primary Model's Output: [Insert the primary model's response here]\n\
Dialogue History: [Insert recent dialogue history here]\n\
\n\
Now generate Neuro-sama's refined response.")
# Load the validation dataset
def load_validation_data(path):
    with open(path, 'r') as file:
        return [json.loads(line) for line in file]

# Run inference with the primary model and save results
def inference_with_primary_model(data, primary_output_path):
    results = []
    for item in data:
        prompt = item['prompt']
        
        # Replace with your actual primary model inference function
        primary_response = primary_model_inference(prompt)  # Placeholder function
        print(primary_response)
        
        results.append({
            'prompt': prompt,
            'primary_response': primary_response,
            'expected_response': item['completion']
        })
    
    # Save the primary model outputs
    with open(primary_output_path, 'w') as outfile:
        for result in results:
            json.dump(result, outfile)
            outfile.write('\n')

# Create a secondary model dataset with the primary model's output
def create_secondary_model_data(primary_output_path, secondary_data_path):
    secondary_data = []
    
    with open(primary_output_path, 'r') as file:
        for line in file:
            data = json.loads(line)
            
            # Extracting the dialogue history and the primary response
            userinputstart = "<|im_start|>user\n"
            dialogue_history = data['prompt'].split(userinputstart)[-1]
            dialogue_history = dialogue_history.split("<|end_header_id|>")[-1]
            dialogue_history = dialogue_history.split("<|eot_id|>")[0]
            primary_response = data['primary_response']
            
            secondary_prompt = secondary_system_prompt.replace("[Insert the primary model's response here]", primary_response)
            secondary_prompt = secondary_prompt.replace("[Insert recent dialogue history here]", dialogue_history)
            
            # Format for secondary model training
            completion = data['expected_response'].strip()
            completion = completion.replace("<|im_end|>", "<|eot_id|>")
            
            # Append the formatted dialogue to the secondary data
            secondary_data.append({
                'prompt': secondary_prompt,
                'completion': completion
            })
    
    # Save the dataset for the secondary model
    with open(secondary_data_path, 'w') as outfile:
        for item in secondary_data:
            json.dump(item, outfile)
            outfile.write('\n')


# Mock function for primary model inference
import subprocess

def primary_model_inference(prompt):
    # Define the command and parameters for mlx_lm.generate
    command = [
        "mlx_lm.generate",  # Command
        "--model", "/Users/skyliu/Downloads/llama3.1 dolphin 8B",  # Model path
        "--adapter-path", "/Users/skyliu/adapters",  # Adapter path
        "--prompt", prompt  # Prompt to pass
    ]

    try:
        # Execute the command and capture the output
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        if result.returncode == 0:
            output = result.stdout
            # Parsing output: Assume the response starts after a specific token
            response_start = "<|im_start|>assistant\n"
            response = output.split(response_start)[-1]  # Get text after the last instance of response_start
            response = response.split("==========")[0]  # Remove text after the response end tag

            # Clean and return the generated response
            return response.strip()

        else:
            print("Error:", result.stderr)
            return "Error generating response."
        # Return the generated response from the command output
    except subprocess.CalledProcessError as e:
        print(f"Error occurred during inference: {e}")
        print(f"Standard output: {e.stdout}")
        print(f"Standard error: {e.stderr}")
        return "Error in generating response."

# Load validation data
validation_data = load_validation_data(valid_data_path)

# Inference with primary model and save output
#inference_with_primary_model(validation_data, primary_output_path)

# Create and save the secondary model dataset
create_secondary_model_data(primary_output_path, secondary_data_path)
